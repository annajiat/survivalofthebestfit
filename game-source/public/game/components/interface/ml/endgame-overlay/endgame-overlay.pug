include ../../ui-overlay/ui-overlay.pug
include ../../../universal/button/button
script(src='../scripts-website.min.js')
+overlay()(id="js-endgame-overlay" class="EndgameOverlay")
	
	.conversationBox
		.conversation-container
			#part1
				h3 Taking A Step Back
				img(src='assets/img/manual-hiring-01.png')

				p As a recruiter, which of these attributes did you value the most?

				#buttons
					div(class="button button__choice" onclick="select()")
						p Education

					div(class="button button__choice" onclick="select()")
						p Experience

					div(class="button button__choice" onclick="select()")
						p Ambition

					div(class="button button__choice" onclick="select()")
						p Skills


			//+button('Edblaon').EndgameOverlay__button
			//+button('Experience').EndgameOverlay__button
			//+button('Ambition').EndgameOverlay__button
			//+button('Skills').EndgameOverlay__button

			#part2

				p(style="float: left") Our data says you valued [ATTRIBUTE] the most - intentionally or unintentionally. Hiring is subjective, and can also often invoke biases and stereotypes; something many people are aware of. Maybe machines can fix that? <br>

				div(class="button button__proceed" onClick="next1()")
					p(style="font-size: 1.2rem") NEXT

			
			#part3
				
				h3 Training the Algorithm

				p Since the algorithm learns from the data it was given, like <b> cv_all.zip </b>, it inherits any biases you might have. Your data alone wasn't enough to build the software, though, because machine learning (ML) only works on large amounts of data! The probem is, these historical datasets can have all sorts of inequalities built in. <br>

				img(src='assets/img/processing.png')

				p In this case, it largely consisted of applicants from Bluetown, where more people historically worked in tech. In the real world, this could mean your data reflects a history of injustice, like favoring a certain race, gender, or socio-economic background. <br> 

				p What if you ask your algorithm to disregard looking at these differentiators? <br>

				div(class="button button__proceed" onClick="next2()")
					p(style="font-size: 1.2rem; color: white") NEXT

			
			#part4

				h3 Behind the Code

				p There is no on-off switch to ask automated decision-making programs to do the right thing. In hiring, for example, it's not just that data is categorized by race or gender, but that it also represents the colleges, societies, and organizations certain demographics were allowed to be a part of. <br>

				img(src='assets/img/automated-hiring-01.png', style="width: 60%; margin-left: 0%; margin-right: auto; clear: right; margin-top: 3%;")

				p When a program learns from years of data, we need to question the data it is learning from. Just because a decision is presented as 'automated' doesn't mean it's right or objective. <br>

				p For more information, visit our resources section! <br>

				div(class="button button__proceed")
					a(href='../resources.html') GO TO WEBSITE
