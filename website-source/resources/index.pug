extends ../layout/layout-default.pug

block content
	script(src='../scripts-website.min.js')
	body(class="resources")
		
		h1 On Socially Responsible AI
		div#info
			div#text1

				h4.header-box MACHINES & BIAS

				p.info-box Bias has long been a problem in recruiting. Research has shown that ref1, ref2, and ref3. One might think that computers can help us eliminate this human bias, but automation is not always the answer. There are several ways bias can be propagate through computer code. Since ‘machine learning’ algorithms work by learning from previous trends, bias could exist if the learning data we give our program is already biased. For example, if you teach a computer to learn from the past century of successful CVs, it can pick on trends that reflect historical and societal biases. <br> <br> As a matter of fact, sometimes the data isn’t biased because it represents a historical reality, it’s also inaccurate. According to research on gender bias in natural language processing (i.e. computers understanding text), ref.

					h4.next-box(onclick='open1()') NEXT: HOW DO WE FIX IT?		
	
			div#text2

				h4.header-box HOW DO WE FIX IT?		

				p.info-box One of the main problems with using software to make important decisions is that you often can’t track down why a decision was made. In machine learning, this is what we call the “black box problem”. A software would learn from data and try to replicate it, but it does not let you know specifically what that decision-making process looks like. In cases where software solutions could provide corporations and governments from accountability, it may be a better option to refrain from using them at all. <br> <br> The solution to bias is not straightforward. It might be easy to think that to fix bias in the dataset, all we need to do is to gather a representational dataset. Since machine learning works on mass data, however, this may still not help minorities. Navigating bias and representation is a complex topic, and so is fairness. If we want to build equitable software systems that do not discriminate against or disadvantage any members of our society, we must open up this conversation to people who are not software engineers, or in the tech industry. 

					h4.next-box(onclick='open2()') NEXT: TAKING STEPS FORWARD
					h4.next-box(style="float: left; margin-left: 5%;" onclick='back()') BACK	


			div#text3

				h4.header-box TAKING STEPS FORWARD

				p.info-box <b> Education and Awareness </b> <br> <br> We believe that the first step to achieving real progress is to engage in research and conversation beyond both within and outside the technical field, such as policy-makers, lawyers, social scientists, and advocacy groups. We need to gain a better understanding of how to navigate fairness, bias, and equity in the digital world. For that to be possible, we must work on bridging the technical gap. We want issues in tech ethics to be accessible to those who may have not taken a computer science class before, but still have a lot to add to the conversation. <br> <br> <b> Ethical Frameworks </b> <br> While we think people outside of tech should be involved, we want tech companies and software developers to make an effort too. This means making discussions about ethics be a conscious part of the software development process. <br> Institute for the Future and Omidyar Network worked on Ethical OS, “a guide to anticipating the future impact of today’s technology”.  <br> AI Now Institute, based at NYU, have also worked on an Algorithmic Impact Assessment guide aimed at public agencies, with the aim of providing a practical framework to assess automated decision systems. TBC

				h4.next-box(style="float: left; margin-left: 5%;" onclick='back()') BACK	


