extends ./layout/layout-default.pug

block content
	script(src='../scripts-website.min.js')
	body(class="resources")
		
		h1 Software, Bias, and Justice
		div#info
			div#text1

				h4.header-box (1/3) BEHIND THE TECH

				p.info-box You don't need to be an engineer to question the tech affecting our lives. The goal isn't necessarily to have everyone start building AI, though the field can certainly use <a style="text-decoration-line: underline; color:#BE5E5E" href='https://medium.com/@AINowInstitute/gender-race-and-power-in-ai-a-playlist-2d3a44e43d3b'> more diversity</a>, but to have enough awareness to join the conversation and ask important questions! <br> <br> <a style="text-decoration-line: underline; color:#BE5E5E" href='https://www.youtube.com/watch?v=z-EtmaFJieY'> Machine learning (ML)</a>, is an application of artificial intelligence (AI) that deals with teaching computers how to make decisions by learning from large amounts of data. This can be powerful and useful, but just because a decisions is 'automated', doesn't mean it has to be correct, objective, or fair. <br> <br> When building automated decision-making systems, we should be asking: what metrics are these decisions based on, and who can we hold accountable? With machine learning, that becomes tricky, because after the program is trained on pre-existing data, finding out <b><i> why </i></b> a certain decision was made isn't straightforward. It's what is referred to as the <b> Black Box problem</b>. <br> <br> Technology should help us move forward, and not repeat the mistakes of the past. If the data we use, for example, represents a history when only people of a certain race, gender, or class are privileged by the system, then what are we teaching our software? 

					a(href='#info')
						h4.next-box(onclick='open1()') FAIR SOFTWARE >
	
			div#text2

				h4.next-box(style="float: left; margin-left: 1.5%; margin-right: 0%;" onclick='back()') <b><</b>

				h4.header-box(style="margin-left: 1%;") (2/3) FAIR SOFTWARE

				p.info-box Feeding ML software disproportionate data, or data that excludes people in our society, would mean that the decisions they produce can abide by negative stereotypes, be modelled on long-standing injustice, or exlude certain groups of people. We want to make sure our technology works for everyone, and doesn't make historic injustices and inequalities <a style="text-decoration-line: underline; color:#BE5E5E" href="https://points.datasociety.net/systemic-algorithmic-harms-e00f99e72c42"> even more systemic</a>. Fairness is subjective, and bias is complicated. We don't expect software engineers to solve it all - which is why we think you should be a part of the conversation. <br> <br> Building software isn't a purely technical challenge when it involves many social, political, and economic implications. Whether it relates to algorithmic bias, or other ways tech is affecting our lives, we need to be able to ask questions, and hold developers accountable. <br> <br> Sometimes, we might have to ask, how did you ensure this software doesn't oppress or exclude? At other times, we ask, is using this technology suitable at all? If a machine learning solution doesn't allow us to question its outcome, should governments and corporations be able to use automated decision-making to absolve themselves from responsibility? 

					a(href='#info')
						h4.next-box(onclick='open2()') STEPS FORWARD >
						


			div#text3

				h4.next-box(style="float: left; margin-left: 1.5%; margin-right: 0%;" onclick='back()') <b><</b>

				h4.header-box(style="margin-left: 1%;") (3/3) STEPS FORWARD

				p.info-box We don't have a shortcut solution to 'fix' algorithmic bias, but we <i>do</i> have many great people working on it. We don't want to reinvent the wheel, but to encourage you to support and expand on ongoing efforts. Building ethical, equitable technology doesn't start or stop at algorithmic bias, which is why we hope the public at large becomes better informed of how technology affects our lives at large. <br><br>We need collaboration among individuals and organiztations across various fields to both take strides forward in building more just technology, and hold those developing it accountable. This is why we want issues in tech ethics to be accessible to those who may have not taken a computer science class before, but still have a lot to add to the conversation. <br><br> There are many angles to approaching fair tech, and they include greater public awareness and advocacy, <a href='https://blog.usejournal.com/from-principles-to-action-how-do-we-implement-tech-ethics-d59aebb05ed8?sk=d01ca6f6d827b44414d34730dd920ab2' style="text-decoration-line: underline; color:#BE5E5E">holding tech companies accountable</a>, engaging more diverse voices, and developing more <a style="text-decoration-line: underline; color:#BE5E5E" href='https://www.fastcompany.com/90355969/want-to-fix-big-tech-change-what-classes-are-required-for-a-computer-science-degree'>socially conscious CS education curricula</a>. <br> <br> If you would like to learn more about algorithmic bias or the impact of technology on our societies, check out our reading list below. Then, we hope you will ask questions, voice your opinions, and support the call for technology that benefits our societies.

			a(href='https://github.com/survivalofthebestfit/survivalofthebestfit/wiki/Reading-List')
				h4#select(style="text-decoration-line: underline; color: #BE5E5E;font-weight: 700; margin-left: 38%; margin-bottom: 5%; clear: left;") <i>GO TO OUR READING LIST</i>

